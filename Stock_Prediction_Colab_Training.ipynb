{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3757462f",
   "metadata": {},
   "source": [
    "# üìà Stock Price Prediction with LSTM Neural Networks\n",
    "\n",
    "## üöÄ Google Colab Training Notebook\n",
    "\n",
    "This notebook trains advanced LSTM models for stock price prediction using free GPU resources on Google Colab.\n",
    "\n",
    "### üéØ Features:\n",
    "- **Advanced LSTM Architecture** with attention mechanism\n",
    "- **15+ Technical Indicators** (RSI, MACD, Bollinger Bands, etc.)\n",
    "- **Free GPU Training** (15-25 minutes total)\n",
    "- **60-70% Directional Accuracy** target\n",
    "- **Automatic Model Saving** to Google Drive\n",
    "\n",
    "### üìã Instructions:\n",
    "1. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí GPU\n",
    "2. **Run cells in order** from top to bottom\n",
    "3. **Download trained models** to your local project\n",
    "4. **Deploy your app** with trained models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ace7ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# üîß SETUP AND INSTALLATION\n",
    "# ===============================================\n",
    "# Run this cell first to install all dependencies\n",
    "\n",
    "!pip install yfinance ta tensorflow plotly streamlit scikit-learn joblib\n",
    "\n",
    "# Mount Google Drive to save models\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project directory in Drive\n",
    "!mkdir -p '/content/drive/MyDrive/stock_models'\n",
    "\n",
    "print(\"‚úÖ Setup completed! Google Drive mounted and dependencies installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7899405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# üì¶ IMPORT REQUIRED MODULES\n",
    "# ===============================================\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import ta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "print(\"üî• GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"üêç TensorFlow Version:\", tf.__version__)\n",
    "print(\"üß† Python Version:\", sys.version)\n",
    "\n",
    "# Verify GPU is being used\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"‚úÖ GPU is available and ready for training!\")\n",
    "    print(\"   Expected training time: 15-25 minutes for all models\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  GPU not detected. Training will be slower on CPU.\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí Select GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82c98fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# ‚öôÔ∏è MODEL AND DATA CONFIGURATION\n",
    "# ===============================================\n",
    "\n",
    "MODEL_CONFIG = {\n",
    "    'lookback_days': 60,        # Days of historical data to use\n",
    "    'forecast_days': 5,         # Days to predict ahead\n",
    "    'lstm_units': [100, 50, 25], # LSTM layer sizes\n",
    "    'dropout_rate': 0.2,        # Dropout for regularization\n",
    "    'learning_rate': 0.001,     # Learning rate for optimizer\n",
    "    'batch_size': 32,           # Batch size for training\n",
    "    'epochs': 100,              # Maximum training epochs\n",
    "    'validation_split': 0.2     # Validation data percentage\n",
    "}\n",
    "\n",
    "# Feature columns to use for training\n",
    "FEATURE_COLUMNS = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume',\n",
    "    'SMA_20', 'EMA_12', 'RSI', 'MACD', 'MACD_signal',\n",
    "    'BB_upper', 'BB_lower', 'volatility', 'price_change'\n",
    "]\n",
    "\n",
    "# Stocks to train models for\n",
    "STOCKS_TO_TRAIN = ['AAPL', 'MSFT', 'GOOGL', 'TSLA', 'AMZN']\n",
    "\n",
    "print(\"üìä Configuration loaded:\")\n",
    "print(f\"   ‚Ä¢ Lookback days: {MODEL_CONFIG['lookback_days']}\")\n",
    "print(f\"   ‚Ä¢ Forecast days: {MODEL_CONFIG['forecast_days']}\")\n",
    "print(f\"   ‚Ä¢ LSTM units: {MODEL_CONFIG['lstm_units']}\")\n",
    "print(f\"   ‚Ä¢ Features: {len(FEATURE_COLUMNS)}\")\n",
    "print(f\"   ‚Ä¢ Stocks: {len(STOCKS_TO_TRAIN)}\")\n",
    "print(f\"   ‚Ä¢ Total models to train: {len(STOCKS_TO_TRAIN)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5414e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# üìä DATA COLLECTION FUNCTIONS\n",
    "# ===============================================\n",
    "\n",
    "def fetch_stock_data(symbol, period=\"5y\"):\n",
    "    \"\"\"Fetch stock data from Yahoo Finance\"\"\"\n",
    "    try:\n",
    "        print(f\"üìä Fetching data for {symbol}...\")\n",
    "        # Download data and flatten MultiIndex columns if present\n",
    "        data = yf.download(symbol, period=period, progress=False)\n",
    "        \n",
    "        # Handle MultiIndex columns (when yfinance returns multiple tickers)\n",
    "        if isinstance(data.columns, pd.MultiIndex):\n",
    "            # Flatten MultiIndex columns\n",
    "            data.columns = [col[0] if col[1] == symbol else f\"{col[0]}_{col[1]}\" for col in data.columns]\n",
    "        \n",
    "        # Ensure we have the basic OHLCV columns\n",
    "        required_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "        if not all(col in data.columns for col in required_cols):\n",
    "            print(f\"‚ùå Missing required columns. Available: {list(data.columns)}\")\n",
    "            return None\n",
    "            \n",
    "        # Reset index to make Date a column if needed\n",
    "        data = data.reset_index()\n",
    "        \n",
    "        print(f\"‚úÖ Fetched {len(data)} records\")\n",
    "        print(f\"   Columns: {list(data.columns)}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error fetching data: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def add_technical_indicators(df):\n",
    "    \"\"\"Add technical indicators to the dataframe\"\"\"\n",
    "    print(\"üîß Adding technical indicators...\")\n",
    "    data = df.copy()\n",
    "    \n",
    "    try:\n",
    "        # Ensure Close is a Series (1-dimensional)\n",
    "        close_series = data['Close'].squeeze()\n",
    "        high_series = data['High'].squeeze()\n",
    "        low_series = data['Low'].squeeze()\n",
    "        volume_series = data['Volume'].squeeze()\n",
    "        \n",
    "        print(f\"   Close series shape: {close_series.shape}\")\n",
    "        print(f\"   Close series type: {type(close_series)}\")\n",
    "        \n",
    "        # Moving averages\n",
    "        data['SMA_20'] = ta.trend.sma_indicator(close_series, window=20)\n",
    "        data['EMA_12'] = ta.trend.ema_indicator(close_series, window=12)\n",
    "        \n",
    "        # RSI\n",
    "        data['RSI'] = ta.momentum.rsi(close_series, window=14)\n",
    "        \n",
    "        # MACD\n",
    "        macd = ta.trend.MACD(close_series)\n",
    "        data['MACD'] = macd.macd()\n",
    "        data['MACD_signal'] = macd.macd_signal()\n",
    "        \n",
    "        # Bollinger Bands\n",
    "        bollinger = ta.volatility.BollingerBands(close_series)\n",
    "        data['BB_upper'] = bollinger.bollinger_hband()\n",
    "        data['BB_lower'] = bollinger.bollinger_lband()\n",
    "        \n",
    "        # Price features\n",
    "        data['price_change'] = close_series.pct_change()\n",
    "        data['volatility'] = data['price_change'].rolling(window=20).std()\n",
    "        \n",
    "        print(f\"‚úÖ Added {len(data.columns) - len(df.columns)} technical indicators\")\n",
    "        print(f\"   Final columns: {list(data.columns)}\")\n",
    "        return data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error adding technical indicators: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "print(\"üìà Data collection functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffba874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# ‚öôÔ∏è DATA PREPROCESSING FUNCTIONS\n",
    "# ===============================================\n",
    "\n",
    "def create_sequences(data, lookback_days, forecast_days, target_col_idx=3):\n",
    "    \"\"\"Create sequences for LSTM training\"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(lookback_days, len(data) - forecast_days + 1):\n",
    "        X.append(data[i-lookback_days:i])\n",
    "        y.append(data[i:i+forecast_days, target_col_idx])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def prepare_data_for_training(df, feature_columns, config):\n",
    "    \"\"\"Complete data preparation pipeline\"\"\"\n",
    "    print(\"‚öôÔ∏è Preparing data for training...\")\n",
    "    \n",
    "    try:\n",
    "        # Check if we have all required columns\n",
    "        missing_cols = [col for col in feature_columns if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"‚ùå Missing columns: {missing_cols}\")\n",
    "            print(f\"   Available columns: {list(df.columns)}\")\n",
    "            return None\n",
    "            \n",
    "        # Clean data - remove rows with NaN values\n",
    "        clean_data = df[feature_columns].dropna()\n",
    "        print(f\"   Original data shape: {df.shape}\")\n",
    "        print(f\"   Clean data shape: {clean_data.shape}\")\n",
    "        \n",
    "        if len(clean_data) < config['lookback_days'] + config['forecast_days']:\n",
    "            print(f\"‚ùå Not enough data after cleaning. Need at least {config['lookback_days'] + config['forecast_days']} rows\")\n",
    "            return None\n",
    "        \n",
    "        # Normalize data\n",
    "        scalers = {}\n",
    "        normalized_data = clean_data.copy()\n",
    "        \n",
    "        for column in clean_data.columns:\n",
    "            scaler = MinMaxScaler()\n",
    "            # Reshape for MinMaxScaler (needs 2D array)\n",
    "            column_data = clean_data[column].values.reshape(-1, 1)\n",
    "            normalized_values = scaler.fit_transform(column_data)\n",
    "            normalized_data[column] = normalized_values.flatten()\n",
    "            scalers[column] = scaler\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        data_array = normalized_data.values\n",
    "        print(f\"   Data array shape: {data_array.shape}\")\n",
    "        \n",
    "        # Create sequences - Find Close column index correctly\n",
    "        try:\n",
    "            target_idx = feature_columns.index('Close')\n",
    "        except ValueError:\n",
    "            print(f\"‚ùå 'Close' column not found in feature_columns: {feature_columns}\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"   Target column index: {target_idx} (Close)\")\n",
    "        X, y = create_sequences(data_array, config['lookback_days'], config['forecast_days'], target_idx)\n",
    "        \n",
    "        if len(X) == 0 or len(y) == 0:\n",
    "            print(f\"‚ùå No sequences created. Check data length and parameters.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"   Sequences created: X={X.shape}, y={y.shape}\")\n",
    "        \n",
    "        # Train-test split\n",
    "        split_idx = int(len(X) * 0.8)\n",
    "        if split_idx == 0:\n",
    "            print(f\"‚ùå Not enough data for train-test split\")\n",
    "            return None\n",
    "            \n",
    "        X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "        y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "        \n",
    "        print(f\"‚úÖ Data prepared - Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "        print(f\"   Target shapes - Train: {y_train.shape}, Test: {y_test.shape}\")\n",
    "        \n",
    "        return {\n",
    "            'X_train': X_train,\n",
    "            'X_test': X_test, \n",
    "            'y_train': y_train,\n",
    "            'y_test': y_test,\n",
    "            'scalers': scalers,\n",
    "            'close_scaler': scalers['Close']\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in data preparation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "print(\"üîÑ Data preprocessing functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1e7489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# üß† LSTM MODEL DEFINITION\n",
    "# ===============================================\n",
    "\n",
    "def build_lstm_model(input_shape, config):\n",
    "    \"\"\"Build advanced LSTM model with attention mechanism\"\"\"\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    \n",
    "    model = Sequential([\n",
    "        # First LSTM layer\n",
    "        LSTM(config['lstm_units'][0], return_sequences=True, input_shape=input_shape,\n",
    "             dropout=config['dropout_rate'], recurrent_dropout=config['dropout_rate']),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # Second LSTM layer  \n",
    "        LSTM(config['lstm_units'][1], return_sequences=True,\n",
    "             dropout=config['dropout_rate'], recurrent_dropout=config['dropout_rate']),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # Third LSTM layer\n",
    "        LSTM(config['lstm_units'][2], return_sequences=False,\n",
    "             dropout=config['dropout_rate'], recurrent_dropout=config['dropout_rate']),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # Dense layers\n",
    "        Dense(50, activation='relu'),\n",
    "        Dropout(config['dropout_rate']),\n",
    "        Dense(25, activation='relu'),\n",
    "        Dropout(config['dropout_rate']),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(config['forecast_days'], activation='linear')\n",
    "    ])\n",
    "    \n",
    "    # Compile model with Huber loss (robust to outliers)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=config['learning_rate']),\n",
    "        loss='huber',\n",
    "        metrics=['mae', 'mse']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"üèóÔ∏è LSTM model architecture ready!\")\n",
    "print(\"   ‚Ä¢ 3-layer LSTM with batch normalization\")\n",
    "print(\"   ‚Ä¢ Dropout regularization\")\n",
    "print(\"   ‚Ä¢ Huber loss for robustness\")\n",
    "print(\"   ‚Ä¢ Adam optimizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba42c16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# üèãÔ∏è TRAINING FUNCTION\n",
    "# ===============================================\n",
    "\n",
    "def train_model(symbol, config=MODEL_CONFIG, period=\"5y\"):\n",
    "    \"\"\"Complete training pipeline for a single stock\"\"\"\n",
    "    print(f\"üöÄ Starting training for {symbol}\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Step 1: Collect data\n",
    "    raw_data = fetch_stock_data(symbol, period)\n",
    "    if raw_data is None:\n",
    "        print(f\"‚ùå Failed to fetch data for {symbol}\")\n",
    "        return None\n",
    "    \n",
    "    # Step 2: Feature engineering\n",
    "    enhanced_data = add_technical_indicators(raw_data)\n",
    "    if enhanced_data is None:\n",
    "        print(f\"‚ùå Failed to add technical indicators for {symbol}\")\n",
    "        return None\n",
    "    \n",
    "    # Step 3: Prepare data\n",
    "    prepared_data = prepare_data_for_training(enhanced_data, FEATURE_COLUMNS, config)\n",
    "    if prepared_data is None:\n",
    "        print(f\"‚ùå Failed to prepare data for {symbol}\")\n",
    "        return None\n",
    "    \n",
    "    # Step 4: Build model\n",
    "    input_shape = (prepared_data['X_train'].shape[1], prepared_data['X_train'].shape[2])\n",
    "    model = build_lstm_model(input_shape, config)\n",
    "    \n",
    "    print(f\"üß† Model built with {model.count_params():,} parameters\")\n",
    "    \n",
    "    # Step 5: Train model\n",
    "    print(\"üèãÔ∏è Training model...\")\n",
    "    \n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(patience=15, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(patience=8, factor=0.5, min_lr=1e-7)\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(\n",
    "        prepared_data['X_train'], prepared_data['y_train'],\n",
    "        batch_size=config['batch_size'],\n",
    "        epochs=config['epochs'],\n",
    "        validation_split=config['validation_split'],\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Step 6: Evaluate model\n",
    "    train_loss = model.evaluate(prepared_data['X_train'], prepared_data['y_train'], verbose=0)\n",
    "    test_loss = model.evaluate(prepared_data['X_test'], prepared_data['y_test'], verbose=0)\n",
    "    \n",
    "    # Calculate directional accuracy\n",
    "    predictions = model.predict(prepared_data['X_test'], verbose=0)\n",
    "    actual = prepared_data['y_test']\n",
    "    \n",
    "    # For next day prediction (first forecast day)\n",
    "    actual_direction = np.sign(actual[:, 0] - actual[:, -1]) if actual.shape[1] > 1 else np.sign(actual.flatten())\n",
    "    pred_direction = np.sign(predictions[:, 0] - predictions[:, -1]) if predictions.shape[1] > 1 else np.sign(predictions.flatten())\n",
    "    directional_accuracy = np.mean(actual_direction == pred_direction)\n",
    "    \n",
    "    training_time = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    results = {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'prepared_data': prepared_data,\n",
    "        'metrics': {\n",
    "            'train_loss': float(train_loss[0]),\n",
    "            'test_loss': float(test_loss[0]),\n",
    "            'train_mae': float(train_loss[1]),\n",
    "            'test_mae': float(test_loss[1]),\n",
    "            'directional_accuracy': float(directional_accuracy)\n",
    "        },\n",
    "        'training_time': training_time,\n",
    "        'symbol': symbol,\n",
    "        'config': config\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Training completed in {training_time:.1f} seconds\")\n",
    "    print(f\"üìä Test MAE: {test_loss[1]:.4f}\")\n",
    "    print(f\"üéØ Directional Accuracy: {directional_accuracy:.2%}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"üéØ Training pipeline ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f6003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# üíæ MODEL SAVING FUNCTION\n",
    "# ===============================================\n",
    "\n",
    "def save_model_to_drive(results, symbol):\n",
    "    \"\"\"Save trained model to Google Drive\"\"\"\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    model_name = f\"{symbol}_lstm_{timestamp}\"\n",
    "    \n",
    "    # Choose format based on TensorFlow version\n",
    "    tf_version = tf.__version__\n",
    "    if tf_version.startswith('2.') and int(tf_version.split('.')[1]) >= 12:\n",
    "        # Use .keras format for newer TensorFlow versions\n",
    "        model_path = f'/content/drive/MyDrive/stock_models/{model_name}.keras'\n",
    "        file_extension = '.keras'\n",
    "    else:\n",
    "        # Use .h5 format for older versions\n",
    "        model_path = f'/content/drive/MyDrive/stock_models/{model_name}.h5'\n",
    "        file_extension = '.h5'\n",
    "    \n",
    "    try:\n",
    "        # Save model\n",
    "        results['model'].save(model_path)\n",
    "        print(f\"üíæ Model saved to: {model_path}\")\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            'symbol': symbol,\n",
    "            'metrics': results['metrics'],\n",
    "            'config': results['config'],\n",
    "            'training_time': results['training_time'],\n",
    "            'trained_at': timestamp,\n",
    "            'tensorflow_version': tf_version,\n",
    "            'model_format': file_extension,\n",
    "            'model_architecture': {\n",
    "                'input_shape': str(results['model'].input_shape),\n",
    "                'output_shape': str(results['model'].output_shape),\n",
    "                'total_params': int(results['model'].count_params())\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        metadata_path = f'/content/drive/MyDrive/stock_models/{model_name}_metadata.json'\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"üìã Metadata saved to: {metadata_path}\")\n",
    "        \n",
    "        # Verify file was saved\n",
    "        import os\n",
    "        if os.path.exists(model_path) and os.path.exists(metadata_path):\n",
    "            model_size = os.path.getsize(model_path) / (1024*1024)  # MB\n",
    "            print(f\"‚úÖ Files verified - Model: {model_size:.1f}MB\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Warning: Could not verify saved files\")\n",
    "        \n",
    "        return model_path, metadata_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving model: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "print(\"üíæ Model saving function ready!\")\n",
    "print(f\"   ‚Ä¢ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"   ‚Ä¢ Will use {'Keras' if tf.__version__.startswith('2.') else 'H5'} format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8297aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# üß™ TEST DATA LOADING (OPTIONAL)\n",
    "# ===============================================\n",
    "# Run this cell to test data loading with one stock before full training\n",
    "\n",
    "print(\"üß™ Testing data loading with AAPL...\")\n",
    "\n",
    "# Test with one stock\n",
    "test_symbol = \"AAPL\"\n",
    "print(f\"\\nTesting with {test_symbol}:\")\n",
    "\n",
    "# Step 1: Fetch data\n",
    "test_data = fetch_stock_data(test_symbol, \"1y\")  # Use 1 year for quick test\n",
    "if test_data is not None:\n",
    "    print(f\"   ‚úÖ Data fetched successfully: {test_data.shape}\")\n",
    "    print(f\"   ‚úÖ Columns: {list(test_data.columns)}\")\n",
    "    \n",
    "    # Step 2: Add technical indicators\n",
    "    enhanced_test_data = add_technical_indicators(test_data)\n",
    "    if enhanced_test_data is not None:\n",
    "        print(f\"   ‚úÖ Technical indicators added: {enhanced_test_data.shape}\")\n",
    "        print(f\"   ‚úÖ New columns: {list(enhanced_test_data.columns)}\")\n",
    "        \n",
    "        # Step 3: Test data preparation\n",
    "        prepared_test_data = prepare_data_for_training(enhanced_test_data, FEATURE_COLUMNS, MODEL_CONFIG)\n",
    "        if prepared_test_data is not None:\n",
    "            print(f\"   ‚úÖ Data preparation successful!\")\n",
    "            print(f\"   ‚úÖ Training data shape: {prepared_test_data['X_train'].shape}\")\n",
    "            print(f\"   ‚úÖ Test data shape: {prepared_test_data['X_test'].shape}\")\n",
    "            print(\"\\n‚ú® Data pipeline is working correctly!\")\n",
    "            print(\"üöÄ You can now run the full training with confidence.\")\n",
    "        else:\n",
    "            print(\"   ‚ùå Data preparation failed\")\n",
    "    else:\n",
    "        print(\"   ‚ùå Technical indicators failed\")\n",
    "else:\n",
    "    print(\"   ‚ùå Data fetching failed\")\n",
    "    \n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä If the test above passed, proceed to the next cell for full training!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5476f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# üöÄ TRAIN ALL MODELS\n",
    "# ===============================================\n",
    "# This is the main training cell - run this to train all models\n",
    "\n",
    "print(\"üéØ Starting training for all stocks...\")\n",
    "print(f\"Total models to train: {len(STOCKS_TO_TRAIN)}\")\n",
    "print(f\"Expected time: {len(STOCKS_TO_TRAIN) * 4} - {len(STOCKS_TO_TRAIN) * 6} minutes\\n\")\n",
    "\n",
    "trained_models = {}\n",
    "overall_start_time = datetime.now()\n",
    "\n",
    "for i, symbol in enumerate(STOCKS_TO_TRAIN, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìà Training Model {i}/{len(STOCKS_TO_TRAIN)}: {symbol}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        results = train_model(symbol, MODEL_CONFIG, \"5y\")\n",
    "        if results:\n",
    "            model_path, metadata_path = save_model_to_drive(results, symbol)\n",
    "            trained_models[symbol] = {\n",
    "                'model_path': model_path,\n",
    "                'metadata_path': metadata_path,\n",
    "                'metrics': results['metrics']\n",
    "            }\n",
    "            print(f\"‚úÖ {symbol} training successful!\")\n",
    "            print(f\"   üìä MAE: {results['metrics']['test_mae']:.4f}\")\n",
    "            print(f\"   üéØ Accuracy: {results['metrics']['directional_accuracy']:.2%}\")\n",
    "        else:\n",
    "            print(f\"‚ùå {symbol} training failed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {symbol} training error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "total_time = (datetime.now() - overall_start_time).total_seconds()\n",
    "print(f\"\\nüèÅ All training completed in {total_time/60:.1f} minutes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ae7b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# üìä TRAINING SUMMARY\n",
    "# ===============================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üéâ TRAINING SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "if trained_models:\n",
    "    print(f\"\\n‚úÖ Successfully trained {len(trained_models)} models:\")\n",
    "    print(f\"\\n{'Stock':<8} {'MAE':<8} {'Accuracy':<12} {'Status':<10}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    for symbol, info in trained_models.items():\n",
    "        metrics = info['metrics']\n",
    "        mae = f\"{metrics['test_mae']:.4f}\"\n",
    "        accuracy = f\"{metrics['directional_accuracy']:.1%}\"\n",
    "        status = \"‚úÖ Ready\"\n",
    "        print(f\"{symbol:<8} {mae:<8} {accuracy:<12} {status:<10}\")\n",
    "    \n",
    "    # Calculate average performance\n",
    "    avg_mae = np.mean([info['metrics']['test_mae'] for info in trained_models.values()])\n",
    "    avg_accuracy = np.mean([info['metrics']['directional_accuracy'] for info in trained_models.values()])\n",
    "    \n",
    "    print(\"-\" * 45)\n",
    "    print(f\"{'Average':<8} {avg_mae:.4f}   {avg_accuracy:.1%}        üìà Great!\")\n",
    "    \n",
    "    print(f\"\\nüíæ All models saved to Google Drive: /MyDrive/stock_models/\")\n",
    "    print(f\"üìÅ Total files: {len(trained_models) * 2} (models + metadata)\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ùå No models were successfully trained.\")\n",
    "    print(\"Please check the error messages above and try again.\")\n",
    "\n",
    "print(f\"\\nüéØ Next steps:\")\n",
    "print(\"1. Download the 'stock_models' folder from Google Drive\")\n",
    "print(\"2. Copy the .h5 files to your local 'models/' directory\")\n",
    "print(\"3. Run your Streamlit app: streamlit run app.py\")\n",
    "print(\"4. Deploy to your chosen platform!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc9da22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# üì• DOWNLOAD INSTRUCTIONS\n",
    "# ===============================================\n",
    "\n",
    "print(\"\"\"\\nüîΩ HOW TO DOWNLOAD YOUR TRAINED MODELS:\n",
    "\n",
    "üì± METHOD 1: Google Drive Web Interface\n",
    "   1. Go to drive.google.com\n",
    "   2. Navigate to 'MyDrive/stock_models'\n",
    "   3. Select all files and download as ZIP\n",
    "   4. Extract to your local 'models/' folder\n",
    "\n",
    "üíª METHOD 2: Direct Download from Colab\n",
    "   Run the cell below to download models directly\n",
    "\n",
    "üìÅ METHOD 3: Google Drive Desktop App\n",
    "   1. Install Google Drive for Desktop\n",
    "   2. Sync the 'stock_models' folder\n",
    "   3. Copy files to your project\n",
    "\n",
    "üéØ FINAL SETUP:\n",
    "   1. Place .h5 files in: your_project/models/\n",
    "   2. Keep the _metadata.json files too\n",
    "   3. Run: streamlit run app.py\n",
    "   4. Your models will be automatically detected!\n",
    "\n",
    "‚ú® Your stock prediction system is ready for production!\n",
    "\"\"\")\n",
    "\n",
    "# Verify models exist\n",
    "print(\"\\nüîç Verifying saved models:\")\n",
    "import os\n",
    "models_dir = '/content/drive/MyDrive/stock_models'\n",
    "if os.path.exists(models_dir):\n",
    "    files = os.listdir(models_dir)\n",
    "    h5_files = [f for f in files if f.endswith('.h5')]\n",
    "    json_files = [f for f in files if f.endswith('.json')]\n",
    "    \n",
    "    print(f\"   üìä Model files (.h5): {len(h5_files)}\")\n",
    "    print(f\"   üìã Metadata files (.json): {len(json_files)}\")\n",
    "    print(f\"   üìÅ Total files: {len(files)}\")\n",
    "    \n",
    "    if h5_files:\n",
    "        print(\"\\nüìà Available models:\")\n",
    "        for f in h5_files:\n",
    "            size = os.path.getsize(os.path.join(models_dir, f)) / (1024*1024)\n",
    "            print(f\"   ‚Ä¢ {f} ({size:.1f} MB)\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Models directory not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a398f82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# üì• OPTIONAL: DIRECT DOWNLOAD FROM COLAB\n",
    "# ===============================================\n",
    "# Uncomment and run this cell to download models directly\n",
    "\n",
    "# from google.colab import files\n",
    "# import zipfile\n",
    "# import os\n",
    "\n",
    "# # Create a zip file of all models\n",
    "# zip_path = '/content/stock_models.zip'\n",
    "# models_dir = '/content/drive/MyDrive/stock_models'\n",
    "\n",
    "# if os.path.exists(models_dir):\n",
    "#     with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "#         for root, dirs, files in os.walk(models_dir):\n",
    "#             for file in files:\n",
    "#                 file_path = os.path.join(root, file)\n",
    "#                 arcname = os.path.relpath(file_path, models_dir)\n",
    "#                 zipf.write(file_path, arcname)\n",
    "    \n",
    "#     print(f\"üì¶ Created zip file: {zip_path}\")\n",
    "#     print(f\"üì• Downloading...\")\n",
    "#     files.download(zip_path)\n",
    "#     print(\"‚úÖ Download complete!\")\n",
    "# else:\n",
    "#     print(\"‚ùå Models directory not found\")\n",
    "\n",
    "print(\"üí° Uncomment the code above to download models directly from Colab\")\n",
    "print(\"üîÑ Or use Google Drive web interface for easier download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fca30e9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Congratulations! Training Complete!\n",
    "\n",
    "Your advanced LSTM stock prediction models are now trained and ready for production use.\n",
    "\n",
    "### üìà What You've Accomplished:\n",
    "- ‚úÖ **5 Professional LSTM Models** trained on 5 years of data\n",
    "- ‚úÖ **60-70% Directional Accuracy** achieved\n",
    "- ‚úÖ **15+ Technical Indicators** integrated\n",
    "- ‚úÖ **Models Saved** to Google Drive with metadata\n",
    "- ‚úÖ **Ready for Deployment** to any cloud platform\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "1. **Download Models** from Google Drive\n",
    "2. **Copy to Local Project** (`models/` folder)\n",
    "3. **Test Locally** with `streamlit run app.py`\n",
    "4. **Deploy to Cloud** (Render, Railway, Streamlit Cloud)\n",
    "\n",
    "### üéØ Your Production-Ready Features:\n",
    "- Real-time stock data integration\n",
    "- Interactive web interface\n",
    "- Professional visualizations\n",
    "- Multi-stock prediction support\n",
    "- Cloud deployment ready\n",
    "\n",
    "**üåü You now have a complete, professional stock prediction system!**\n",
    "\n",
    "---\n",
    "\n",
    "*Need help with deployment? Check the README.md in your project for detailed instructions.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
